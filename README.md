# psicoapp
PsicoApp - A GEI UdL UCD project


## Arranca Ollama (`http://localhost:11434/`)
Descarga y arranca el modelo (tardará un poco si no lo tienes)

    ollama run llama3

Una vez descargado, asegúrate de que Ollama esté corriendo en segundo plano (normalmente en `localhost:11434`).

## Backend (`http://localhost:3001/`)

    cd backend
    npm install
    npm run dev


## Frontend (`http://localhost:5173/`)

    cd frontend
    npm install
    npm run dev
